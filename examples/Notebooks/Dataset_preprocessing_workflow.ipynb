{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charged-administrator",
   "metadata": {},
   "source": [
    "# HydroGeoSines\n",
    "## A general data processing workflow\n",
    "\n",
    "This notebook demonstrates the general data handling capabilities of HydroGeoSines. The standard workflow for loading, processing and analysing data, as well as exporting and visualizing results is demonstrated on a simple example dataset. We show how the Site object and its methods can be used to store data and how the data processing is handled via the Processing object and its methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-andrews",
   "metadata": {},
   "source": [
    "### Import HGS\n",
    "Currently, the HydroGeoSines is not fully implemented as an installable package. Instead. we have to move to the parent directory, to import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excellent-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /home/olivier/work/chyn/volet4/projects/tidal_analysis/lib/HydroGeoSines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "# Load the HGS package\n",
    "import hydrogeosines as hgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reverse-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and other packages used in this tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-valuation",
   "metadata": {},
   "source": [
    "### The Site object\n",
    "Typically, we have time series data of groundwater head measurements from a couple of different loggers that are located at a site of interest. Similarly, we aggreate all our data records into a hgs.Site object. The Site object has a geo-location that attribute to add information on longitude, latitude and height . This is can later be used to calculate site specific Earth Tide records.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hydrogeosines.models.site.Site object at 0x775847923a50>\n"
     ]
    }
   ],
   "source": [
    "# Create a Site object\n",
    "example_site = hgs.Site('example', geoloc=[141.762065, -31.065781, 160])\n",
    "print(example_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-liberal",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "#### Import groundwater head records\n",
    "The import_csv method of the Site object can be used to import the three standard input categories \"GW\", \"BP\" and \"ET\" (groundwater, barometric pressure, and earth tides). In general, the hgs package is implemented in SI units. By passing a *unit* argument for your input dataset, units are automatically converted. \n",
    "\n",
    "In the present example, a dataset with three groundwater records is loaded. The location names are explicitly set as \"Loc_A\", \"Loc_B\" and \"Loc_C\" using the loc_names parameter, because there are no column headers in the data set (header = None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "included-attack",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/work/chyn/volet4/projects/tidal_analysis/lib/HydroGeoSines/hydrogeosines/models/ext/read.py:67: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S%z format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  data = pd.read_csv(filepath, parse_dates=True, index_col=0, dayfirst=dayfirst, header = header, names=loc_names, usecols=usecols)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "utcoffset(dt) argument must be a datetime instance or None, not DatetimeIndex",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load all our data attributed to the Site\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mexample_site\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtests/data/notebook/GW_record.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[43minput_category\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mutc_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mloc_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoc_A\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoc_B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcheck_duplicates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/work/chyn/volet4/projects/tidal_analysis/lib/HydroGeoSines/hydrogeosines/models/ext/read.py:86\u001b[0m, in \u001b[0;36mRead.import_csv\u001b[0;34m(self, filepath, input_category, utc_offset, unit, how, loc_names, header, check_duplicates, dayfirst, dt_format)\u001b[0m\n\u001b[1;32m     84\u001b[0m data\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m d\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# check if dt is \"naive\":\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d\u001b[38;5;241m.\u001b[39mtzinfo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtzinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutcoffset\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# make UTC correction\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetime was \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaive\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Localized and converted to UTC!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     data\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtz_localize(tz\u001b[38;5;241m=\u001b[39mpytz\u001b[38;5;241m.\u001b[39mFixedOffset(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m60\u001b[39m\u001b[38;5;241m*\u001b[39mutc_offset)))\u001b[38;5;241m.\u001b[39mtz_convert(pytz\u001b[38;5;241m.\u001b[39mutc)\n",
      "\u001b[0;31mTypeError\u001b[0m: utcoffset(dt) argument must be a datetime instance or None, not DatetimeIndex"
     ]
    }
   ],
   "source": [
    "# Load all our data attributed to the Site\n",
    "example_site.import_csv('tests/data/notebook/GW_record.csv', \n",
    "                        input_category=[\"GW\"]*2, \n",
    "                        utc_offset=10, \n",
    "                        unit=[\"m\"]*2,\n",
    "                        loc_names = [\"Loc_A\",\"Loc_B\"], \n",
    "                        header = None,\n",
    "                        check_duplicates=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-pharmacology",
   "metadata": {},
   "source": [
    "The Site object now has the groundwater records added to its data attribute. It is stored as a Pandas DataFrame with a set of predefined column names:\n",
    " - **datetime:** the first column of every input data record should be a datetime convertible format\n",
    " - **category:** the data category (GW,BP or ET)\n",
    " - **location:** either infered from the header or defined by the loc_names parameter of the import method\n",
    " - **part:** pre-set to \"all\". For non-uniform data records, the data set is later split into uniform parts\n",
    " - **unit:** unit (SI after import)\n",
    " - **value** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_site.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_site.data.location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-tension",
   "metadata": {},
   "source": [
    "#### Import barometric pressure records\n",
    "The import of barometric pressure records is similar to the groundwater head import. Only \"BP\" needs to be passed as an argument to the \"category\" parameter. Setting the *how* parameter to \"all\", the Site data attribute is updated and the BP record is added to the previously imported GW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_site.import_csv('tests/data/notebook/BP_record.csv', \n",
    "                        input_category=\"BP\", \n",
    "                        utc_offset=10, \n",
    "                        unit=\"m\", \n",
    "                        loc_names = \"Baro\",\n",
    "                        header = None,\n",
    "                        how=\"add\", check_duplicates=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_site.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-lewis",
   "metadata": {},
   "source": [
    "### The Processing object\n",
    "The Processing object enables easy access to the hgs methods for data pre-processing and data analysis. These include methods for calculating barometric efficiencies, corrected groundwater heads or extracting harmonic components from records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Processing object of example site\n",
    "process_example = hgs.Processing(example_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9a925",
   "metadata": {},
   "source": [
    "#### Get comprehensive info on the data loaded into the  processing object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25194f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_example.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-enclosure",
   "metadata": {},
   "source": [
    "After instantiating the Processing object, we can simply run the desired method, which returns a new object containing the method results. In this case, we want to compute all available time domain barometric efficiencies (BE) available in the BE_time() method. \n",
    "\n",
    "#### Example: Calculate BE using the BE_time() method\n",
    "The BE_time() methods requires our data to be uniformly sampled. Thus, preprocessing steps are applied to the data of the Site object. First the groundwater head measurements are resampled, interpolated and if necessary split into sub-parts of uniform sampling. Then the BP records are aligned with the GW data. Then the barometric efficiencies are calculated for every location and part individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the BE Time methods\n",
    "BE_results  = process_example.BE_time(method=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-activation",
   "metadata": {},
   "source": [
    "#### The results container\n",
    "BE_results now contains a <font color=\"green\">nested</font> dictionary for the <font color=\"red\">*BE_time()*</font> method results. The top level of the nested dictionary constains one item for each method that has been applied to the data. The second level contains one item for each location and its sub-parts.\n",
    "\n",
    "- Each method is stored as an item in the results dictionary with the name of the method as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BE_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d75d7e",
   "metadata": {},
   "source": [
    "- The method dictionary items are also dictionaries (i.e. forming a nested dictionary). For each location a seperate entry is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341945c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BE_results[\"be_time\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb04c5",
   "metadata": {},
   "source": [
    " - The final method results are stored as a list with 3 entries. The first entry (index 0) contains the method output, the second (1) the input data as a DataFrame with Datetime index, and the third entry (2) is for additional information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output:\\n\",BE_results[\"be_time\"][\"Loc_A\",\"1\"][0],\"\\n\")\n",
    "print(\"Input:\\n\", BE_results[\"be_time\"][\"Loc_A\",\"1\"][1].head(3),\"\\n\")\n",
    "print(\"Info:\\n\", BE_results[\"be_time\"][\"Loc_A\",\"1\"][2],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-kitchen",
   "metadata": {},
   "source": [
    "#### How to filter data by groundwater location\n",
    "Once we created our Site object containing all our data, we can decide to process only a subset of the available locations, using the gw_loc method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Processing object for only one specific groundwater location of example_site\n",
    "locations = [\"Loc_A\"]\n",
    "process_A = hgs.Processing(example_site).by_gwloc(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e0757",
   "metadata": {},
   "source": [
    "Lets check if there is now only the data of location A (\"Loc_A\") in our processing object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4355df",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_A.site.data.location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68758aa4",
   "metadata": {},
   "source": [
    "### Advanced and manual preprocessing\n",
    "Although the Processing class automatically handles and applies all required and recommended data preprocessing steps for the analysis methods to work, these can also be customized by the user. \n",
    "\n",
    "The RegularAndAligned() method consists of two main functions. First, the make_regular() method to regularly sample the groundwater data and second, the BP_align() method to align the BP entries to the groundwater data. As a result, every groundwater record will have a matching BP meassurement for the same point in time.\n",
    "\n",
    "####  The make_regular() method\n",
    "The make_regular() method can be accessed directly through the hgs pandas accessor:\n",
    "```python\n",
    "example_site.data.hgs.make_regular()\n",
    "```\n",
    "\n",
    "It has several parameters with default values:\n",
    " - **inter_max:** int = 3600 <br />*This is the maximum interpolated time interval in seconds. Any gap larger than this value will not be interpolated.*\n",
    " - **part_min:** int = 20 <br />*The minimum record duration without gaps in days. If there are gaps in the data that can not be interpolated, the data is split into parts. TIn this case, every part needs to fullfill the minimum criteria. Otherwise it is dropped from the data.* \n",
    " - **method:** str = \"backfill\" <br />*The interpolation method of Pandas to be used. Check out the Pandas documenation for more informations on the available methods.*\n",
    " - **category** = \"GW\" <br />*This method was developed for groundwater data, but can in principal be applied to other categories as well.*\n",
    " - **spl_freq:** int = None <br />*The method is automatically calculating the most common sampling frequency for each location. But the parameter can also be passed to the function as an argument.*\n",
    " - **inter_max_total:** int = 10 <br />*The maximum percentage threshold of values to be interpolated. If this threshold is exceeded, there were to many gaps in the data.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data from site object\n",
    "data = example_site.data\n",
    "\n",
    "# lets check if there are any NaN in the value column of the groundwater category:\n",
    "print(\"There are missing values in the data:\", data[data.category == \"GW\"].value.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f6911",
   "metadata": {},
   "source": [
    "##### Upsample data\n",
    "Now lets upsample (i.e. interpolate the missing values) our data using the \"time\" method. Internally this calls on the following function, which is individually applied to all locations of the GW data:\n",
    "```python\n",
    "data.hgs.upsample(\"time\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6baa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resample = data.hgs.make_regular(method='time', inter_max = 3600)\n",
    "data_resample.hgs.filters.get_gw_data.head(3)\n",
    "\n",
    "#, part_min = 20, method = \"backfill\", category = \"GW\", spl_freq = None, inter_max_total = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16d3ad",
   "metadata": {},
   "source": [
    "##### Custom sampling frequency\n",
    "Resample data to a sampling frequency of 1 hour (3600 seconds).\n",
    "\n",
    "**Careful!** The interpolation maximum (inter_max) always has to be equal or higher than the sampling frequency. Otherwise your data won't be interpolated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dea4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resample = data.hgs.make_regular(inter_max = 5400, spl_freq = 3600)\n",
    "data_resample.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f43475",
   "metadata": {},
   "source": [
    "Our data has been resampled to one sample per hour. HGS also provides a DataFrame attribute to check for the most common sample frequency by group (i.e. splitted by category, location, parts and unit) called *spl_freq_groupby*. This attribute is also accessed by the make_regular() method and used to configure the resampling.\n",
    "\n",
    "We can see that all GW data is resampled to **3600**, while the BP data was untouched and still has a sampling frequency of **300**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spl_freq = data_resample.hgs.spl_freq_groupby\n",
    "spl_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb9c2c",
   "metadata": {},
   "source": [
    "We can also use the information from this HGS attribute and redefine the sampling frequencies, which can then be passed on to other methods. Lets say we want the sampling frequency of our groundwater records for locations **Loc_A** and **Loc_B** to be **180** and **1500**, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f2d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sampling frequencies of each group\n",
    "spl_freq = data.hgs.spl_freq_groupby\n",
    "# redefine the sampling frequencies for the groundwater category\n",
    "spl_freq[\"GW\"] = [180,1500]\n",
    "print(spl_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b93822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample all data\n",
    "custom_resample = data.hgs.resample_by_group(spl_freq, origin=\"start\")\n",
    "# Get BP and GW data in seperate DataFrames\n",
    "bp_data = custom_resample.hgs.filters.get_bp_data\n",
    "gw_data = custom_resample.hgs.filters.get_gw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d2ff1",
   "metadata": {},
   "source": [
    "Looking at the groundwater data of location B we can see that it is now resampled to one sample every 25 minutes (1500 seconds). Additionally, the origin was set to the original start time of the record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_data[gw_data.location==\"Loc_B\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6ae84",
   "metadata": {},
   "source": [
    "Our groundwater data for location A is sampled every 3 minutes (180 seconds). Of course, resampling the data at a frequency higher then the original 300 seconds leaves us with gaps at regular intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_data[gw_data.location==\"Loc_A\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecd6ab",
   "metadata": {},
   "source": [
    "#### Interpolate gaps\n",
    "The make_regular() method of HGS applies a gap filling routine to the data using groupby. It can be accessed via:\n",
    "```python\n",
    "hgs.ext.pandas_hgs.HgsAccessor.gap_routine\n",
    "```\n",
    "\n",
    "We usually apply this data processing step to only one of our data categories (GW or BP). In our case, we choose to fill the gaps in our resampled GW data. Thus, we select the required sampling frequency information accordingly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the BP sampling frequency information\n",
    "spl_freqs_gw = spl_freq.drop(\"BP\")\n",
    "spl_freqs_gw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b6e4d",
   "metadata": {},
   "source": [
    "First we group our gw_data DataFrame by its columns of Dtype *object*, which are \"category\", \"location\", \"part\" and \"unit\" by default. HGS provides a simple filtering attribute for this operation:\n",
    "```python\n",
    "gw_data.hgs.filters.obj_col\n",
    "```\n",
    "\n",
    "Then we apply the gap_routine setting the following parameters:\n",
    "- **spl_freqs_gw:** These are our custom groundwater sampling frequencies of our data. They are used to identify gaps that might be too large for interpolation. This parameter is not strictly neccessary, as the gap routine also checks the HGS *spl_freq_groupby* attribute. \n",
    "- **inter_max_total:** We need to interpolate a lot of data, thus we have to increase or interpolation maximum.\n",
    "- **part_min** \n",
    "- **method:** Interpolate the value column using the datetime as index. This results in a smooth interpolation between existing datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797df63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular = gw_data.groupby(gw_data.hgs.filters.obj_col).apply(hgs.ext.pandas_hgs.HgsAccessor.gap_routine,mcf=spl_freqs_gw,inter_max_total=50,part_min=10,method=\"time\") \n",
    "regular = regular.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168003eb",
   "metadata": {},
   "source": [
    "As we can see from the console print out, around 40% of all groundwater data at Location A had to be interpolated. Now, lets have a closer look at the data. We can see that location A has been split into three parts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd55f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular.hgs.filters.loc_names_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the pivot table we can see all parts of the location at once\n",
    "regular[regular.location==\"Loc_A\"].hgs.pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular[regular.location==\"Loc_B\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c48d4",
   "metadata": {},
   "source": [
    "Lets see if there are any NaN values left in the groundwater data. Or in other words, there are no records without a valid value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular.hgs.filters.is_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c7468",
   "metadata": {},
   "source": [
    "NICE!!! Our data is regularly sampled and interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a37b83",
   "metadata": {},
   "source": [
    "####  The BP_align() method\n",
    "The BP_align() method can be accessed directly through the hgs pandas accessor:\n",
    "```python\n",
    "example_site.data.hgs.BP_align()\n",
    "```\n",
    "\n",
    "It has several parameters with default values, all of them can also be found in the make_regular() method:\n",
    "- **inter_max:** int = 3600\n",
    "- **method:** str = \"backfill\"\n",
    "- **part_min:** int = 20\n",
    "- **inter_max_total:** int = 10\n",
    "\n",
    "BP_align() automatically tries to match the sampling frequency of the groundwater records. It does so, by individually upsampling or downsampling the BP records for each GW location and its parts. Therefore, no user defined sampling frequency is available at this step. \n",
    "Gaps that exceed the inter_max threshold and thus, can not be interpolated are used to drop the according entries from the GW record. This generally causes another split into parts. The part_min parameter ensures that only parts large then the threshold are retained in the data.\n",
    "In some cases the BP and GW data can not be aligned. The main reason usually is that there are too many gaps in the BP record. In this case, try to reduce the part_min or increase the inter_max and inter_max_total parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c09160",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aligned = data_resample.hgs.BP_align(inter_max = 5400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d85d4",
   "metadata": {},
   "source": [
    "We can now check if the GW and BP data is truely aligned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aligned.hgs.check_alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436666c",
   "metadata": {},
   "source": [
    "At this stage it can be very convenient for inspection to pivot our data for and use the datetime as our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aligned.hgs.pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-sheep",
   "metadata": {},
   "source": [
    "#### Add the data_regular attribute to the processing object\n",
    "BE_time and other methods require the data to be uniformly sampled. Thus, if multiple methods need access to uniformly sampled data it sometimes makes sense to pre-process the data using the make_regular() method to reduce the overall processing time.\n",
    "\n",
    "You can also specify additional parameter that are internally passed to the make_regular() and BP_align() method. A comprehensive explanation for both methods and their parameters was given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Processing object of example site\n",
    "process_RAA = hgs.Processing(example_site).RegularAndAligned(inter_max=5000, part_min=20,inter_max_total=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685f78b",
   "metadata": {},
   "source": [
    "Now we have an attribute with the regularly sampled data added to our processing object which can be accessed manually and will automatically be used by processing methods such as BE_time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_RAA.data_regular.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346c6fd",
   "metadata": {},
   "source": [
    "#### Compare runtime\n",
    "Time difference between running the BE_time with a precalculated data_regular() attribute using the RegularAndAligned method() and without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_example = hgs.Processing(example_site)\n",
    "# Turn off console output for readability\n",
    "with hgs.utils.nullify_output(suppress_stdout=True, suppress_stderr=True):\n",
    "    time1 = %timeit -n1 -r1 -o process_RAA.BE_time(method=\"all\")\n",
    "    time2 = %timeit -n1 -r1 -o process_example.BE_time(method=\"all\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With RAA:\",time1)\n",
    "print(\"Without RAA:\",time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-commerce",
   "metadata": {},
   "source": [
    "### The View object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443130f",
   "metadata": {},
   "source": [
    "... under preparation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidal_analysis",
   "language": "python",
   "name": "tidal_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
